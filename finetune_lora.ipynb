{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a464e8c-7899-4a9b-ae69-7bcaec989252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    assert config.n_embd % config.n_head == 0\n",
    "    # key, query, value projections for all heads, but in a batch\n",
    "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "    # regularization\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embd = config.n_embd\n",
    "    # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size,config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.size() # batch size, sequene length, embedding dimensionality\n",
    "    # calculate query key, values for all heads in a batch and move head forwrad to be\n",
    "    # nh is \"number of heads\", hs is \"head size\", and C(number of chanels) = nh * hs\n",
    "    # e.g., in GPT-2 (124M), n_head, hs = 64, so nh*hs = C = 768 channels in the Transformer\n",
    "    qkv = self.c_attn(x)\n",
    "    q, k, v = qkv.split(self.n_embd, dim = 2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "    # att = (q @ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
    "    # att = att.masked_fill(self.bias[:,:,:T,:T]==0, float('-inf'))\n",
    "    # att = F.softmax(att, dim=-1)\n",
    "    # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    y = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "    y = y.transpose(1,2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "    #ouput projection\n",
    "    y = self.c_proj(y)\n",
    "    return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "    self.gelu   = nn.GELU(approximate='tanh')\n",
    "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.c_fc(x)\n",
    "    x = self.gelu(x)\n",
    "    x = self.c_proj(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "    self.attn = CausalSelfAttention(config)\n",
    "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "    self.mlp = MLP(config)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.attn(self.ln_1(x))\n",
    "    x = x + self.mlp(self.ln_2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "  block_size: int = 1024 # max sequence length\n",
    "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
    "  n_layer: int = 12 #number of layers\n",
    "  n_head: int = 12 # number of heads\n",
    "  n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "\n",
    "    self.transformer = nn.ModuleDict(dict(\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "        ln_f = nn.LayerNorm(config.n_embd),\n",
    "    ))\n",
    "\n",
    "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    # weight sharing scheme\n",
    "    self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    # init params\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      std = 0.02\n",
    "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "        std *= (2*self.config.n_layer)** 0.5\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "      if module.bias is not None:\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "  def forward(self, idx, targets = None):\n",
    "    #idx is of shape (B, T)\n",
    "    B, T = idx.size()\n",
    "    assert T <= self.config.block_size, f\"Cannot forward sequence of length{T}, block_size is {self.config.block_size}\"\n",
    "\n",
    "    #forward the token and position embeddings\n",
    "    pos = torch.arange(0, T, dtype = torch.long, device = idx.device) # shape (T)\n",
    "    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "    tok_emb = self.transformer.wte(idx) # token embeddings of shae (B, T, n_embd)\n",
    "    x = tok_emb + pos_emb\n",
    "    # forward the blocks of the transformer\n",
    "    for block in self.transformer.h:\n",
    "      x = block(x)\n",
    "    # forward the final layernorm and the classifier\n",
    "    x = self.transformer.ln_f(x)\n",
    "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9c643b-4751-4edf-8f99-dcb5964c9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, enc, filename):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open(filename, 'r') as f:\n",
    "            text = f.read()\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2279fee-e310-496b-8d4d-afa1d73a7aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99c7d0e-8bce-437a-90fd-2b4eca03c8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# attempt to autodetect the device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdcc053-fd3d-4981-a3be-d7a89d498458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a huggingface/transformers model\\\n",
    "# model_type = 'gpt2'\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fd36f3-3b04-4356-b27e-d89777bcdd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46514/1876236373.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded at step 15000 with validation loss 3.1931183338165283\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "checkpoint_path = 'model/gpt2_model.pt'\n",
    "# Load the checkpoint dictionary\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Define GPTConfig so it can be found during unpickling\n",
    "model = GPT(checkpoint['config'])\n",
    "# Get the original state dict from the checkpoint\n",
    "state_dict = checkpoint['model']\n",
    "\n",
    "# Remove the '_orig_mod.' prefix from each key\n",
    "new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# Now load the updated state dict into your model\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(f\"Checkpoint loaded at step {checkpoint['step']} with validation loss {checkpoint['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711bf277-48f4-4b07-afa0-915509df0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix tokens\n",
    "model.eval()\n",
    "num_return_sequences = 5\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"We need to save the Planet.\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
    "x = tokens.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fb651c-70a7-4f2f-8288-c77cd131e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We need to save the Planet.\n",
      "The most important part of it is simply our collective responsibility to create a healthy environment and live with dignity and respect and with dignity.\n",
      "However, what kind of environment is the problem that we can make the greatest\n",
      "> We need to save the Planet.”\n",
      "The world’s largest asteroid called 2017 Vesta, a giant, Earth-sized asteroid that’s the largest such impact globally. The asteroid is expected to be 5.5 to 7\n",
      "> We need to save the Planet.” Our concern is a resounding “climate change problem.”\n",
      "What we need is a ‘climate change solution’ that is designed to help the planet and its inhabitants as a whole come\n",
      "> We need to save the Planet. We’re not talking to cars or dirty factories, but to be on guard against toxic pollution.\n",
      "“We’re using technologies to take advantage of all the green options. And we’\n",
      "> We need to save the Planet. The biggest problem will be getting more green energy that turns into power. “I don’t know for sure; I see people as if they have the big idea in their heads.”\n",
      "More\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt_tokens):\n",
    "    x = prompt_tokens.clone()\n",
    "    # generate! right now x is (B, T) where B = 5, T = 8\n",
    "    # set the seed to 42\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    while x.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x) # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "    \n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = x[i, :max_length].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(\">\", decoded)\n",
    "\n",
    "prompt_tokens = tokens.to(device)\n",
    "generate(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7573198-49ba-4ab0-ad4a-c25903b07194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "#########################################\n",
    "# 1) Define your LoRAParametrization\n",
    "#########################################\n",
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, out_features, in_features, rank=1, alpha=1.0, device=None):\n",
    "        super().__init__()\n",
    "        self.lora_B = nn.Parameter(torch.zeros((out_features, rank), device=device))\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank, in_features), device=device))\n",
    "        nn.init.normal_(self.lora_A, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, base_weight: torch.Tensor) -> torch.Tensor:\n",
    "        if self.enabled:\n",
    "            lora_update = torch.matmul(self.lora_B, self.lora_A)\n",
    "            return base_weight + self.scale * lora_update\n",
    "        else:\n",
    "            return base_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dba432-ec39-4693-b300-cabc489f3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_attention(model, rank=4, alpha=8.0, device=None):\n",
    "    for name, module in model.named_modules():\n",
    "        # Check:\n",
    "        # 1) Is this an nn.Linear?\n",
    "        # 2) Does the name contain \"attn\" and \"c_proj\"? \n",
    "        #    (So we wrap only the attention's c_proj, not the MLP's c_proj.)\n",
    "        if isinstance(module, nn.Linear) and (\"attn\" in name) and ((\"c_proj\" in name) or (\"c_attn\" in name)):\n",
    "            out_features, in_features = module.weight.shape\n",
    "            \n",
    "            # Register your LoRAParametrization on 'weight'\n",
    "            parametrize.register_parametrization(\n",
    "                module,\n",
    "                \"weight\",\n",
    "                LoRAParametrization(\n",
    "                    out_features, \n",
    "                    in_features, \n",
    "                    rank=rank, \n",
    "                    alpha=alpha, \n",
    "                    device=device\n",
    "                )\n",
    "            )\n",
    "def enable_disable_lora(enabled=True):\n",
    "  for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and (\"attn\" in name) and ((\"c_proj\" in name) or (\"c_attn\" in name)):\n",
    "            module.parametrizations[\"weight\"][0].enabled = enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a122c12-a406-4914-91c4-87b0f62931bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_lora_to_attention(model, rank=4, alpha=8.0, device='cuda')\n",
    "enable_disable_lora(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76069fd8-6fef-4272-800c-1cd20df72610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA parameters: 221184\n",
      "Non-LoRA parameters: 124475904\n"
     ]
    }
   ],
   "source": [
    "def count_lora_and_non_lora_params(model):\n",
    "    lora_params = 0\n",
    "    non_lora_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # Skip parameters that require no grad if you only want trainable counts\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "\n",
    "        if \"lora_A\" in name or \"lora_B\" in name:\n",
    "            lora_params += param.numel()\n",
    "        else:\n",
    "            non_lora_params += param.numel()\n",
    "\n",
    "    return lora_params, non_lora_params\n",
    "\n",
    "\n",
    "lora_count, non_lora_count = count_lora_and_non_lora_params(model)\n",
    "print(f\"LoRA parameters: {lora_count}\")\n",
    "print(f\"Non-LoRA parameters: {non_lora_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61281797-779a-404a-9b8f-f17950076b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parametertransformer.wte.weight\n",
      "Freezing non-LoRA parametertransformer.wpe.weight\n",
      "Freezing non-LoRA parametertransformer.h.0.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.0.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.0.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.0.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.0.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.0.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.0.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.0.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.0.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.0.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.0.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.0.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.1.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.1.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.1.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.1.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.1.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.1.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.1.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.1.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.1.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.1.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.1.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.1.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.2.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.2.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.2.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.2.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.2.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.2.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.2.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.2.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.2.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.2.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.2.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.2.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.3.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.3.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.3.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.3.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.3.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.3.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.3.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.3.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.3.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.3.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.3.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.3.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.4.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.4.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.4.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.4.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.4.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.4.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.4.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.4.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.4.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.4.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.4.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.4.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.5.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.5.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.5.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.5.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.5.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.5.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.5.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.5.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.5.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.5.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.5.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.5.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.6.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.6.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.6.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.6.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.6.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.6.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.6.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.6.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.6.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.6.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.6.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.6.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.7.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.7.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.7.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.7.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.7.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.7.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.7.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.7.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.7.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.7.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.7.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.7.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.8.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.8.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.8.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.8.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.8.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.8.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.8.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.8.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.8.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.8.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.8.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.8.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.9.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.9.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.9.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.9.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.9.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.9.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.9.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.9.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.9.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.9.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.9.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.9.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.10.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.10.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.10.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.10.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.10.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.10.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.10.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.10.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.10.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.10.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.10.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.10.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.11.ln_1.weight\n",
      "Freezing non-LoRA parametertransformer.h.11.ln_1.bias\n",
      "Freezing non-LoRA parametertransformer.h.11.attn.c_attn.bias\n",
      "Freezing non-LoRA parametertransformer.h.11.attn.c_attn.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.11.attn.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.h.11.attn.c_proj.parametrizations.weight.original\n",
      "Freezing non-LoRA parametertransformer.h.11.ln_2.weight\n",
      "Freezing non-LoRA parametertransformer.h.11.ln_2.bias\n",
      "Freezing non-LoRA parametertransformer.h.11.mlp.c_fc.weight\n",
      "Freezing non-LoRA parametertransformer.h.11.mlp.c_fc.bias\n",
      "Freezing non-LoRA parametertransformer.h.11.mlp.c_proj.weight\n",
      "Freezing non-LoRA parametertransformer.h.11.mlp.c_proj.bias\n",
      "Freezing non-LoRA parametertransformer.ln_f.weight\n",
      "Freezing non-LoRA parametertransformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "# freeze the non-Lora parameters\n",
    "for name, param in model.named_parameters():\n",
    "  if 'lora' not in name:\n",
    "    print(f'Freezing non-LoRA parameter{name}')\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d994dbe1-2671-469b-ab4c-6eb407e20391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: transformer.h.0.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.0.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.0.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.0.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.1.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.1.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.1.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.1.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.2.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.2.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.2.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.2.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.3.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.3.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.3.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.3.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.4.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.4.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.4.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.4.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.5.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.5.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.5.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.5.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.6.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.6.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.6.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.6.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.7.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.7.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.7.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.7.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.8.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.8.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.8.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.8.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.9.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.9.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.9.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.9.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.10.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.10.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.10.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.10.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.11.attn.c_attn.parametrizations.weight.0.lora_B torch.Size([2304, 4])\n",
      "Trainable: transformer.h.11.attn.c_attn.parametrizations.weight.0.lora_A torch.Size([4, 768])\n",
      "Trainable: transformer.h.11.attn.c_proj.parametrizations.weight.0.lora_B torch.Size([768, 4])\n",
      "Trainable: transformer.h.11.attn.c_proj.parametrizations.weight.0.lora_A torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"Trainable:\", n, p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73aa8e3d-54cd-4242-804a-04b95525a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 11286 tokens\n",
      "1 epoch = 5 batches\n",
      "step 0, train loss: 4.258481502532959\n",
      "step 1, train loss: 4.251651287078857\n",
      "step 2, train loss: 4.200958251953125\n",
      "step 3, train loss: 4.242818832397461\n",
      "step 4, train loss: 4.189047336578369\n",
      "step 5, train loss: 4.203182220458984\n",
      "step 6, train loss: 4.193680286407471\n",
      "step 7, train loss: 4.142091274261475\n",
      "step 8, train loss: 4.182614326477051\n",
      "step 9, train loss: 4.130834579467773\n",
      "step 10, train loss: 4.143932819366455\n",
      "step 11, train loss: 4.135772228240967\n",
      "step 12, train loss: 4.082915306091309\n",
      "step 13, train loss: 4.120965003967285\n",
      "step 14, train loss: 4.069603443145752\n",
      "step 15, train loss: 4.079800128936768\n",
      "step 16, train loss: 4.074286460876465\n",
      "step 17, train loss: 4.019215106964111\n",
      "step 18, train loss: 4.058935642242432\n",
      "step 19, train loss: 4.006374835968018\n",
      "step 20, train loss: 4.0183634757995605\n",
      "step 21, train loss: 4.012117862701416\n",
      "step 22, train loss: 3.9580013751983643\n",
      "step 23, train loss: 3.9965455532073975\n",
      "step 24, train loss: 3.945070266723633\n",
      "step 25, train loss: 3.958138942718506\n",
      "step 26, train loss: 3.949782609939575\n",
      "step 27, train loss: 3.8973000049591064\n",
      "step 28, train loss: 3.9351987838745117\n",
      "step 29, train loss: 3.882955312728882\n",
      "step 30, train loss: 3.89738130569458\n",
      "step 31, train loss: 3.887415885925293\n",
      "step 32, train loss: 3.8366806507110596\n",
      "step 33, train loss: 3.8727328777313232\n",
      "step 34, train loss: 3.8194761276245117\n",
      "step 35, train loss: 3.8366687297821045\n",
      "step 36, train loss: 3.8245019912719727\n"
     ]
    }
   ],
   "source": [
    "# Test with LoRA enabled\n",
    "enable_disable_lora(enabled=True)\n",
    "\n",
    "train_filename = \"data/train.txt\"\n",
    "\n",
    "B = 4\n",
    "T = 512\n",
    "train_loader = DataLoaderLite(B=B, T=T, enc=enc, filename=train_filename)\n",
    "\n",
    "# optimize!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "grad_accum_steps = 8\n",
    "num_steps = 300 // grad_accum_steps\n",
    "\n",
    "loss_accum = 0.0\n",
    "model.train()\n",
    "\n",
    "for step in range(num_steps):    \n",
    "    # training loop\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0 \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "           logits, loss = model(x, y)\n",
    "        #import code; code.interact(local=locals())\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "  \n",
    "    print(f\"step {step}, train loss: {loss_accum.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1e8dfd-3009-4908-b1bb-59c994bae3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We need to save the Planet.\n",
      "The next time we got the hell out of the Earth, look in.\n",
      "This is my favorite of the three!\n",
      "I guess it is gonna pop up like this.\n",
      "Yeah, the Earth's just\n",
      "> We need to save the Planet.”\n",
      "The Earth is going green. The atmosphere just got dirty. I don’t get much of anything.\n",
      "I’ve become a big fan of the Universe. Can you trust us?\n",
      "> We need to save the Planet.” Here, God's great lords, He's the Lord!\n",
      "The planet is not the planet.\n",
      "- The planet is not the planet\n",
      "- The planet is not\n",
      "- The moon is not the\n",
      "> We need to save the Planet. The better the planet, the more of the life we have left. It is pretty important to save the Planet and everyone else in it.\n",
      "Do you think the only way to reduce carbon emissions from the planet that\n",
      "> We need to save the Planet. I can’t go to those big numbers (I didn’t want the planet to go bad on Earth); I was scared that’s not it. And there’s no way I\n"
     ]
    }
   ],
   "source": [
    "generate(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cdc2a-8ffc-404d-b6c8-54f8674e5207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7406a61-737d-4f5c-8ba6-9e2de69149f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
