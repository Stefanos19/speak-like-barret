{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a464e8c-7899-4a9b-ae69-7bcaec989252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    assert config.n_embd % config.n_head == 0\n",
    "    # key, query, value projections for all heads, but in a batch\n",
    "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "    # regularization\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embd = config.n_embd\n",
    "    # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size,config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.size() # batch size, sequene length, embedding dimensionality\n",
    "    # calculate query key, values for all heads in a batch and move head forwrad to be\n",
    "    # nh is \"number of heads\", hs is \"head size\", and C(number of chanels) = nh * hs\n",
    "    # e.g., in GPT-2 (124M), n_head, hs = 64, so nh*hs = C = 768 channels in the Transformer\n",
    "    qkv = self.c_attn(x)\n",
    "    q, k, v = qkv.split(self.n_embd, dim = 2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "    # att = (q @ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
    "    # att = att.masked_fill(self.bias[:,:,:T,:T]==0, float('-inf'))\n",
    "    # att = F.softmax(att, dim=-1)\n",
    "    # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    y = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "    y = y.transpose(1,2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "    #ouput projection\n",
    "    y = self.c_proj(y)\n",
    "    return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "    self.gelu   = nn.GELU(approximate='tanh')\n",
    "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.c_fc(x)\n",
    "    x = self.gelu(x)\n",
    "    x = self.c_proj(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "    self.attn = CausalSelfAttention(config)\n",
    "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "    self.mlp = MLP(config)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.attn(self.ln_1(x))\n",
    "    x = x + self.mlp(self.ln_2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "  block_size: int = 1024 # max sequence length\n",
    "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
    "  n_layer: int = 12 #number of layers\n",
    "  n_head: int = 12 # number of heads\n",
    "  n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "\n",
    "    self.transformer = nn.ModuleDict(dict(\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "        ln_f = nn.LayerNorm(config.n_embd),\n",
    "    ))\n",
    "\n",
    "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    # weight sharing scheme\n",
    "    self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    # init params\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      std = 0.02\n",
    "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "        std *= (2*self.config.n_layer)** 0.5\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "      if module.bias is not None:\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "  def forward(self, idx, targets = None):\n",
    "    #idx is of shape (B, T)\n",
    "    B, T = idx.size()\n",
    "    assert T <= self.config.block_size, f\"Cannot forward sequence of length{T}, block_size is {self.config.block_size}\"\n",
    "\n",
    "    #forward the token and position embeddings\n",
    "    pos = torch.arange(0, T, dtype = torch.long, device = idx.device) # shape (T)\n",
    "    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "    tok_emb = self.transformer.wte(idx) # token embeddings of shae (B, T, n_embd)\n",
    "    x = tok_emb + pos_emb\n",
    "    # forward the blocks of the transformer\n",
    "    for block in self.transformer.h:\n",
    "      x = block(x)\n",
    "    # forward the final layernorm and the classifier\n",
    "    x = self.transformer.ln_f(x)\n",
    "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9c643b-4751-4edf-8f99-dcb5964c9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, enc, filename):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open(filename, 'r') as f:\n",
    "            text = f.read()\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2279fee-e310-496b-8d4d-afa1d73a7aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99c7d0e-8bce-437a-90fd-2b4eca03c8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# attempt to autodetect the device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdcc053-fd3d-4981-a3be-d7a89d498458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a huggingface/transformers model\\\n",
    "# model_type = 'gpt2'\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fd36f3-3b04-4356-b27e-d89777bcdd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46466/1876236373.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded at step 15000 with validation loss 3.1931183338165283\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "checkpoint_path = 'model/gpt2_model.pt'\n",
    "# Load the checkpoint dictionary\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Define GPTConfig so it can be found during unpickling\n",
    "model = GPT(checkpoint['config'])\n",
    "# Get the original state dict from the checkpoint\n",
    "state_dict = checkpoint['model']\n",
    "\n",
    "# Remove the '_orig_mod.' prefix from each key\n",
    "new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# Now load the updated state dict into your model\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(f\"Checkpoint loaded at step {checkpoint['step']} with validation loss {checkpoint['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711bf277-48f4-4b07-afa0-915509df0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix tokens\n",
    "model.eval()\n",
    "num_return_sequences = 5\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"We need to save the Planet.\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
    "x = tokens.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fb651c-70a7-4f2f-8288-c77cd131e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We need to save the Planet.\n",
      "The most important part of it is simply our collective responsibility to create a healthy environment and live with dignity and respect and with dignity.\n",
      "However, what kind of environment is the problem that we can make the greatest\n",
      "> We need to save the Planet.”\n",
      "The world’s largest asteroid called 2017 Vesta, a giant, Earth-sized asteroid that’s the largest such impact globally. The asteroid is expected to be 5.5 to 7\n",
      "> We need to save the Planet.” Our concern is a resounding “climate change problem.”\n",
      "What we need is a ‘climate change solution’ that is designed to help the planet and its inhabitants as a whole come\n",
      "> We need to save the Planet. We’re not talking to cars or dirty factories, but to be on guard against toxic pollution.\n",
      "“We’re using technologies to take advantage of all the green options. And we’\n",
      "> We need to save the Planet. The biggest problem will be getting more green energy that turns into power. “I don’t know for sure; I see people as if they have the big idea in their heads.”\n",
      "More\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt_tokens):\n",
    "    x = prompt_tokens.clone()\n",
    "    # generate! right now x is (B, T) where B = 5, T = 8\n",
    "    # set the seed to 42\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    while x.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x) # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "    \n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = x[i, :max_length].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(\">\", decoded)\n",
    "\n",
    "prompt_tokens = tokens.to(device)\n",
    "generate(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73aa8e3d-54cd-4242-804a-04b95525a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 11286 tokens\n",
      "1 epoch = 5 batches\n",
      "step 0, train loss: 4.258481502532959\n",
      "step 1, train loss: 8.337913513183594\n",
      "step 2, train loss: 7.40045166015625\n",
      "step 3, train loss: 5.0925140380859375\n",
      "step 4, train loss: 4.210970878601074\n",
      "step 5, train loss: 3.6672849655151367\n",
      "step 6, train loss: 3.327726364135742\n",
      "step 7, train loss: 3.0555899143218994\n",
      "step 8, train loss: 2.759721279144287\n",
      "step 9, train loss: 2.458864688873291\n",
      "step 10, train loss: 2.1804306507110596\n",
      "step 11, train loss: 1.914075255393982\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"data/train.txt\"\n",
    "\n",
    "B = 4\n",
    "T = 512\n",
    "train_loader = DataLoaderLite(B=B, T=T, enc=enc, filename=train_filename)\n",
    "\n",
    "# optimize!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "grad_accum_steps = 8\n",
    "num_steps = 100 // grad_accum_steps\n",
    "\n",
    "loss_accum = 0.0 \n",
    "for step in range(num_steps):    \n",
    "    # training loop\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0 \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "           logits, loss = model(x, y)\n",
    "        #import code; code.interact(local=locals())\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "   \n",
    "# model.train()\n",
    "    \n",
    "    # x, y = train_loader.next_batch()\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # _, loss = model(x, y)\n",
    "    # loss = loss / accumulation_steps  # Scale loss for accumulation\n",
    "    # loss_accum += loss.detach()\n",
    "\n",
    "    # loss.backward()\n",
    "    # if (step + 1) % accumulation_steps == 0:\n",
    "    #     optimizer.step()\n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    print(f\"step {step}, train loss: {loss_accum.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1e8dfd-3009-4908-b1bb-59c994bae3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We need to save the Planet.\n",
      "The first thing to happen in a natural disaster is a natural disaster. But, it's our own fault. Our guard is in no place at all.\n",
      "No wonder. So save the Planet! Don't\n",
      "> We need to save the Planet. He really looks like he's over for a while. How are you feeling?\n",
      "The hell's goin' on?\n",
      "The Planet won't gimme no more. What's gonna happen to the Planet\n",
      "> We need to save the Planet. I'll start the War!!\n",
      "An' you okay? That damn gun?\n",
      "Why you guys act like that!\n",
      "You damn idea?\n",
      "This makes you a great leader, huh? Don't you get\n",
      "> We need to save the Planet.\n",
      "Let's go!\n",
      "You go get us all alone.\n",
      "Yeah.........that's right!\n",
      "The two guys in charge of the Geys...do it!\n",
      "How the hell can they do...\n",
      "> We need to save the Planet. So you gotta understand that this Planet would be... and what would happen to the People.\n",
      "So let's drive this huge monster like this...\n",
      "It's gonna run with these huge machines...\n",
      "Whoa!\n"
     ]
    }
   ],
   "source": [
    "generate(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cdc2a-8ffc-404d-b6c8-54f8674e5207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb389047-6981-4610-8638-94403d5ebe84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
